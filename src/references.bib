@inproceedings{Chen2021CADA,
	title     = { CADA: Communication-Adaptive Distributed Adam },
	author    = {Chen, Tianyi and Guo, Ziye and Sun, Yuejiao and Yin, Wotao},
	booktitle = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
	pages     = {613--621},
	year      = {2021},
	editor    = {Banerjee, Arindam and Fukumizu, Kenji},
	volume    = {130},
	series    = {Proceedings of Machine Learning Research},
	month     = {13--15 Apr},
	publisher = {PMLR},
	pdf       = {http://proceedings.mlr.press/v130/chen21a/chen21a.pdf},
	url       = {https://proceedings.mlr.press/v130/chen21a.html},
	abstract  = { Stochastic gradient descent (SGD) has taken the stage as the primary workhorse for largescale machine learning. It is often used with its adaptive variants such as AdaGrad, Adam, and AMSGrad. This paper proposes an adaptive stochastic gradient descent method for distributed machine learning, which can be viewed as the communicationadaptive counterpart of the celebrated Adam method — justifying its name CADA. The key components of CADA are a set of new rules tailored for adaptive stochastic gradients that can be implemented to save communication upload. The new algorithms adaptively reuse the stale Adam gradients, thus saving communication, and still have convergence rates comparable to original Adam. In numerical experiments, CADA achieves impressive empirical performance in terms of total communication round reduction. }
}

@misc{Fan2019,
	doi       = {10.48550/ARXIV.1904.05526},
	url       = {https://arxiv.org/abs/1904.05526},
	author    = {Fan, Jianqing and Ma, Cong and Zhong, Yiqiao},
	keywords  = {Machine Learning (stat.ML), Machine Learning (cs.LG), Statistics Theory (math.ST), Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
	title     = {A Selective Overview of Deep Learning},
	publisher = {arXiv},
	year      = {2019},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ADAMKingma,
	doi       = {10.48550/ARXIV.1412.6980},
	url       = {https://arxiv.org/abs/1412.6980},
	author    = {Kingma, Diederik P. and Ba, Jimmy},
	keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title     = {Adam: A Method for Stochastic Optimization},
	publisher = {arXiv},
	year      = {2014},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Ruder2016,
	author     = {Sebastian Ruder},
	title      = {An overview of gradient descent optimization algorithms},
	journal    = {CoRR},
	volume     = {abs/1609.04747},
	year       = {2016},
	url        = {http://arxiv.org/abs/1609.04747},
	eprinttype = {arXiv},
	eprint     = {1609.04747},
	timestamp  = {Mon, 13 Aug 2018 16:48:10 +0200},
	biburl     = {https://dblp.org/rec/journals/corr/Ruder16.bib},
	bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{Jin2021Comet,
	doi       = {10.48550/ARXIV.2111.09562},
	url       = {https://arxiv.org/abs/2111.09562},
	author    = {Jin, Sian and Zhang, Chengming and Jiang, Xintong and Feng, Yunhe and Guan, Hui and Li, Guanpeng and Song, Shuaiwen Leon and Tao, Dingwen},
	keywords  = {Artificial Intelligence (cs.AI), Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title     = {COMET: A Novel Memory-Efficient Deep Learning Training Framework by Using Error-Bounded Lossy Compression},
	publisher = {arXiv},
	year      = {2021},
	copyright = {Creative Commons Attribution 4.0 International}
}

@article{LeCun2015,
	doi       = {10.1038/nature14539},
	url       = {https://doi.org/10.1038/nature14539},
	year      = {2015},
	month     = may,
	publisher = {Springer Science and Business Media {LLC}},
	volume    = {521},
	number    = {7553},
	pages     = {436--444},
	author    = {Yann LeCun and Yoshua Bengio and Geoffrey Hinton},
	title     = {Deep learning},
	journal   = {Nature}
}

@misc{he2015deep,
	title         = {Deep Residual Learning for Image Recognition},
	author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	year          = {2015},
	eprint        = {1512.03385},
	archiveprefix = {arXiv},
	primaryclass  = {cs.CV}
}

@article{Ben-Nun2019Parrallel,
	author     = {Ben-Nun, Tal and Hoefler, Torsten},
	title      = {Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis},
	year       = {2019},
	issue_date = {July 2020},
	publisher  = {Association for Computing Machinery},
	address    = {New York, NY, USA},
	volume     = {52},
	number     = {4},
	issn       = {0360-0300},
	url        = {https://doi.org/10.1145/3320060},
	doi        = {10.1145/3320060},
	abstract   = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
	journal    = {ACM Comput. Surv.},
	month      = {aug},
	articleno  = {65},
	numpages   = {43},
	keywords   = {parallel algorithms, distributed computing, Deep learning}
}

@inproceedings{benchmark2021schmidt,
	title     = {Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers},
	author    = {Schmidt, Robin M and Schneider, Frank and Hennig, Philipp},
	booktitle = {Proceedings of the 38th International Conference on Machine Learning},
	pages     = {9367--9376},
	year      = {2021},
	editor    = {Meila, Marina and Zhang, Tong},
	volume    = {139},
	series    = {Proceedings of Machine Learning Research},
	month     = {18--24 Jul},
	publisher = {PMLR},
	pdf       = {http://proceedings.mlr.press/v139/schmidt21a/schmidt21a.pdf},
	url       = {https://proceedings.mlr.press/v139/schmidt21a.html},
	abstract  = {Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of fifteen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing more than 50,000 individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we cannot discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific optimizers and parameter choices that generally lead to competitive results in our experiments: Adam remains a strong contender, with newer methods failing to significantly and consistently outperform it. Our open-sourced results are available as challenging and well-tuned baselines for more meaningful evaluations of novel optimization methods without requiring any further computational efforts.}
}

@article{JiangDistributed,
	author  = {Jiang, Jiawei and Cui, Bin and Zhang, Ce},
	journal = {Big Data Management},
	title   = {Distributed {Machine} {Learning} and {Gradient} {Optimization}},
	year    = {2022}
}

@misc{Chen2022Efficient,
	doi       = {10.48550/ARXIV.2205.14473},
	url       = {https://arxiv.org/abs/2205.14473},
	author    = {Chen, Congliang and Shen, Li and Liu, Wei and Luo, Zhi-Quan},
	keywords  = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
	title     = {Efficient-Adam: Communication-Efficient Distributed Adam with Complexity Analysis},
	publisher = {arXiv},
	year      = {2022},
	copyright = {Creative Commons Attribution 4.0 International}
}

@online{xiao2017fashion,
	author      = {Han Xiao and Kashif Rasul and Roland Vollgraf},
	title       = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
	date        = {2017-08-28},
	year        = {2017},
	eprintclass = {cs.LG},
	eprinttype  = {arXiv},
	eprint      = {cs.LG/1708.07747}
}

@article{Li2022Federated,
	author  = {Li, Yuhao and Li, Wenling and Zhang, Bin and Du, Junping},
	journal = {IEEE Internet of Things Journal},
	title   = {Federated Adam-Type Algorithm for Distributed Optimization With Lazy Strategy},
	year    = {2022},
	volume  = {9},
	number  = {20},
	pages   = {20519-20531},
	doi     = {10.1109/JIOT.2022.3175997}
}

@misc{Chen2018LAG,
	doi       = {10.48550/ARXIV.1805.09965},
	url       = {https://arxiv.org/abs/1805.09965},
	author    = {Chen, Tianyi and Giannakis, Georgios B. and Sun, Tao and Yin, Wotao},
	keywords  = {Machine Learning (stat.ML), Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
	title     = {LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning},
	publisher = {arXiv},
	year      = {2018},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@techreport{krizhevsky2009cifar,
	author      = {Alex Krizhevsky},
	institution = {},
	title       = {Learning Multiple Layers of Features from Tiny Images},
	year        = {2009}
}

@inproceedings{Horvoth2022Natural,
	title     = {Natural Compression for Distributed Deep Learning},
	author    = {Horv\'{o}th, Samuel and Ho, Chen-Yu and Horvath, Ludovit and Sahu, Atal Narayan and Canini, Marco and Richtarik, Peter},
	booktitle = {Proceedings of Mathematical and Scientific Machine Learning},
	pages     = {129--141},
	year      = {2022},
	editor    = {Dong, Bin and Li, Qianxiao and Wang, Lei and Xu, Zhi-Qin John},
	volume    = {190},
	series    = {Proceedings of Machine Learning Research},
	month     = {15--17 Aug},
	publisher = {PMLR},
	pdf       = {https://proceedings.mlr.press/v190/horvoth22a/horvoth22a.pdf},
	url       = {https://proceedings.mlr.press/v190/horvoth22a.html},
	abstract  = {Modern deep learning models are often trained in parallel over a collection of distributed machines to reduce training time. In such settings, communication of model updates among machines becomes a significant performance bottleneck and various lossy update compression techniques have been proposed to alleviate this problem. In this work, we introduce a new, simple yet theoretically and practically effective compression technique: {\em natural compression ($C_{\text{nat}}$)}. Our technique is applied individually to all entries of the to-be-compressed update vector and works by randomized rounding to the nearest (negative or positive) power of two, which can be computed in a “natural” way by ignoring the mantissa. We show that compared to no compression, $C_{\text{nat}}$ increases the second moment of the compressed vector by not more than the tiny factor $\frac{9}{8}$, which means that the effect of $C_{\text{nat}}$ on the convergence speed of popular training algorithms, such as distributed SGD, is negligible. However, the communications savings enabled by $C_{\text{nat}}$ are substantial, leading to {\em $3$-$4\times$ improvement in overall theoretical running time}. For applications requiring more aggressive compression, we generalize $C_{\text{nat}}$ to {\em natural dithering}, which we prove is {\em exponentially better} than the common random dithering technique. Our compression operators can be used on their own or in combination with existing operators for a more aggressive combined effect, and offer new state-of-the-art both in theory and practice.}
}

@article{hinton2012neural,
	title   = {Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
	author  = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
	journal = {Cited on},
	volume  = {14},
	number  = {8},
	pages   = {2},
	year    = {2012}
}

@misc{Choi2019,
	doi       = {10.48550/ARXIV.1910.05446},
	url       = {https://arxiv.org/abs/1910.05446},
	author    = {Choi,  Dami and Shallue,  Christopher J. and Nado,  Zachary and Lee,  Jaehoon and Maddison,  Chris J. and Dahl,  George E.},
	keywords  = {Machine Learning (cs.LG),  Machine Learning (stat.ML),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
	title     = {On Empirical Comparisons of Optimizers for Deep Learning},
	publisher = {arXiv},
	year      = {2019},
	copyright = {arXiv.org perpetual,  non-exclusive license}
}

@article{qian1999momentum,
	title     = {On the momentum term in gradient descent learning algorithms},
	author    = {Qian, Ning},
	journal   = {Neural networks},
	volume    = {12},
	number    = {1},
	pages     = {145--151},
	year      = {1999},
	publisher = {Elsevier}
}

@misc{Idelbayev18a,
	author       = {Yerlan Idelbayev},
	title        = {Proper {ResNet} Implementation for {CIFAR10/CIFAR100} in {PyTorch}},
	howpublished = {\url{https://github.com/akamaster/pytorch_resnet_cifar10}},
	url          = {https://github.com/akamaster/pytorch_resnet_cifar10},
	year         = {2019},
	note         = {Diakses: 2023-04-19}
}

@article{Chen2021Quantized,
	author     = {Chen, Congliang and Shen, Li and Huang, Haozhi and Liu, Wei},
	title      = {Quantized Adam with Error Feedback},
	year       = {2021},
	issue_date = {October 2021},
	publisher  = {Association for Computing Machinery},
	address    = {New York, NY, USA},
	volume     = {12},
	number     = {5},
	issn       = {2157-6904},
	url        = {https://doi.org/10.1145/3470890},
	doi        = {10.1145/3470890},
	abstract   = {In this article, we present a distributed variant of an adaptive stochastic gradient method for training deep neural networks in the parameter-server model. To reduce the communication cost among the workers and server, we incorporate two types of quantization schemes, i.e., gradient quantization and weight quantization, into the proposed distributed Adam. In addition, to reduce the bias introduced by quantization operations, we propose an error-feedback technique to compensate for the quantized gradient. Theoretically, in the stochastic nonconvex setting, we show that the distributed adaptive gradient method with gradient quantization and error feedback converges to the first-order stationary point, and that the distributed adaptive gradient method with weight quantization and error feedback converges to the point related to the quantized level under both the single-worker and multi-worker modes. Last, we apply the proposed distributed adaptive gradient methods to train deep neural networks. Experimental results demonstrate the efficacy of our methods.},
	journal    = {ACM Trans. Intell. Syst. Technol.},
	month      = {sep},
	articleno  = {56},
	numpages   = {26},
	keywords   = {Adam, quantized communication, error feedback}
}

@misc{Wright2021Ranger,
	doi       = {10.48550/ARXIV.2106.13731},
	url       = {https://arxiv.org/abs/2106.13731},
	author    = {Wright, Less and Demeure, Nestor},
	keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.6},
	title     = {Ranger21: a synergistic deep learning optimizer},
	publisher = {arXiv},
	year      = {2021},
	copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{Wen2017Terngrad,
	doi       = {10.48550/ARXIV.1705.07878},
	url       = {https://arxiv.org/abs/1705.07878},
	author    = {Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
	keywords  = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.6; I.5.1},
	title     = {TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning},
	publisher = {arXiv},
	year      = {2017},
	copyright = {arXiv.org perpetual, non-exclusive license}
}
