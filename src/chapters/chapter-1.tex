\chapter{Pendahuluan}

\section{Latar Belakang}
\label{sec:latarbelakang}

Penggunaan teknik pembelajaran mesin pada saat ini banyak digunakan untuk menyelesaikan permasalahan dalam berbagai domain. Salah satu model pembelajaran mesin yang saat ini banyak digunakan adalah \emph{deep learning}. Proses dalam \emph{deep learning} meliputi pemrosesan atas \emph{dataset} yang dimiliki, kemudian membangun sebuah fungsi stokastik yang mampu memprediksi data yang tidak terdapat di \emph{dataset}.

Sebuah model \emph{deep learning} dibagi atas beberapa \emph{layer}, dan setiap \emph{layer} memiliki parameter yang dapat dioptimasi. Algoritma untuk mengoptimasi parameter\+parameter tersebut disebut sebagai \emph{backpropagation}, karena adanya rentetan pembaruan dari \emph{layer} terakhir hingga \emph{layer} pertama secara berurutan. Optimasi dari parameter-parameter tersebut akan menghasilkan fungsi stokastik yang mendekati fungsi sebenarnya dalam memprediksi \emph{dataset}.

Algoritma \emph{backpropagation} menggunakan gradien dari fungsi objektif untuk mengoptimasi parameter\+parameter. Algoritma pengoptimasi \emph{gradient descent} menggunakan hanya gradien untuk mengoptimasi parameter. Akibatnya, pengoptimasi tersebut mudah terjebak pada minimum lokal. Oleh sebab itu, perlu adanya faktor lain untuk membantu pengoptimasi agar tidak terjebak dalam minimum lokal.

Salah satu pengoptimasi yang umum digunakan dalam \emph{backpropagation} adalah Adam. Pengoptimasi Adam menggunakan gradien orde pertama dengan estimasi momen. Estimasi momen yang digunakan dapat membantu Adam untuk mencapai konvergensi lebih cepat, serta menghindari minimum lokal. Pengoptimasi tersebut memiliki kinerja yang baik pada permasalahan yang tidak konveks.

Dalam praktiknya, pembelajaran mesin dapat dilakukan dengan menjalankan perhitungan\+perhitungan yang sudah ditentukan. Namun, karena jumlah perhitungan dalam \emph{deep learning} meningkat dengan bertambahnya parameter, maka dibutuhkan suatu cara untuk mempercepat pembelajaran. Salah satu teknik awal yang dilakukan adalah dengan melakukan paralelisasi perhitungan menggunakan unit pemrosesan grafis atau GPU. Sebuah GPU mampu melakukan komputasi paralel dengan cepat dan efisien, sehingga cocok untuk melakukan perhitungan \emph{deep learning}.

Seiring berkembangnya model\+model \emph{deep learning}, semakin banyak parameter yang dimiliki model. Akibatnya, paralelisasi pada GPU saja tetap membutuhkan waktu yang signifikan untuk melakukan optimasi parameter. Oleh karena itu, paralelisasi kemudian dilakukan pada beberapa mesin dengan GPU terpisah. Dengan melakukan pembelajaran pada beberapa mesin, parameter dapat dioptimasi dengan lebih cepat.

Selain masalah jumlah parameter, ukuran \emph{dataset} yang digunakan juga menjadi faktor dalam pembelajaran. Saat ini, tersedia beragam \emph{dataset} yang memiliki ukuran besar. Sebuah GPU memiliki jumlah memori yang terbatas, sehingga tidak mungkin memuat keseluruhan \emph{dataset}. Oleh karena itu, akan lebih baik apabila jumlah \emph{dataset} dapat dikurangi dengan membaginya ke beberapa mesin.

Paralelisasi perhitungan dalam \emph{deep learning} dapat mempercepat proses pembelajaran yang dilakukan. Oleh karena itu, banyak penelitian yang meninjau optimasi dalam menggunakan sistem terdistribusi. Aspek penting yang ditinjau adalah mengurangi jumlah komunikasi sehingga mesin tidak banyak menunggu. Salah satu bagian yang mempengaruhi jumlah komunikasi antar \emph{worker} adalah pengoptimasi yang digunakan serta aturan pembaruan bobot.

Penelitian dari \textcite{Chen2021CADA} memberikan alternatif pembaruan bobot dalam lingkungan terdistribusi dengan menggunakan kriteria tertentu untuk menentukan \emph{worker} yang harus mengirimkan parameter dan \emph{worker} yang tidak perlu mengirimkan parameter. Aturan pembaruan ini dapat mengurangi jumlah komunikasi yang diperlukan, namun tetap menjamin konvergensi model. Akibatnya, pembelajaran dapat dilakukan dengan lebih efisien. Teknik ini diberi tajuk CADA, "Communication-Adaptive Distributed Adam."

Penelitian lain dari \textcite{Chen2022Efficient} memberikan alternatif untuk mengurangi beban komunikasi dengan mengurangi data yang dikirimkan. Ukuran data dikurangi dengan cara melakukan kuantisasi. Untuk mengatasi masalah bias, \textcite{Chen2022Efficient} menggunakan \emph{error\+feedback}. Akibatnya, beban komunikasi dapat dikurangi. Teknik ini diberi tajuk Efficient-Adam.

\section{Rumusan Masalah}
Algoritma pengoptimasi Adam dalam lingkungan terdistribusi memiliki banyak modifikasi. Beberapa yang telah disebutkan yakni algoritma dari \textcite{Chen2021CADA} serta \textcite{Chen2022Efficient}. Algoritma dari \textcite{Chen2021CADA} mengoptimasi komunikasi dengan mengurangi jumlah komunikasi. Algoritma dari \textcite{Chen2022Efficient} mengoptimasi komunikasi dengan mengurangi ukuran data yang dikirimkan saat komunikasi.

Belum ada modifikasi pengoptimasi Adam yang secara bersamaan mengurangi jumlah komunikasi dan ukuran data yang dikirimkan. Maka, rumusan masalah dari tugas akhir ini ialah:

\begin{enumerate}
    \item Bagaimana cara memodifikasi pengoptimasi Adam pada lingkungan terdistribusi sehingga ukuran data dan jumlah komunikasi dapat dikurangi?
    \item Bagaimana perbandingan jumlah komunikasi dari modifikasi tersebut terhadap algoritma dari \textcite{Chen2021CADA}, yakni CADA?
    \item Bagaimana perbandingan ukuran data dari modifikasi tersebut terhadap algoritma dari \textcite{Chen2022Efficient}, yakni EfficientAdam?
\end{enumerate}

\section{Tujuan}

Berdasarkan rumusan masalah yang telah ditulis pada subbab sebelumnya, tujuan dari tugas akhir ini ialah:
\begin{enumerate}
    \item Melakukan modifikasi terhadap pengoptimasi Adam pada lingkungan terdistribusi untuk mengurangi ukuran data dan jumlah komunikasi.
    \item Melakukan analisis komparatif dengan algoritma dari \textcite{Chen2021CADA}.
    \item Melakukan analisis komparatif dengan algoritma dari \textcite{Chen2022Efficient}.
\end{enumerate}

\section{Metodologi}

Metodologi yang digunakan dalam tugas akhir ini ialah:
\begin{enumerate}
    \item Mempelajari metode yang digunakan pada penelitian \textcite{Chen2021CADA} dan \textcite{Chen2022Efficient}.
    \item Pengembangan modifikasi Adam menggunakan teknik yang digunakan pada penelitian \textcite{Chen2021CADA} dan \textcite{Chen2022Efficient}.
    \item Melakukan implementasi terhadap modifikasi yang dibuat.
    \item Melakukan pengujian terhadap implementasi.
\end{enumerate}

% \section{Jadwal Pelaksanaan Tugas Akhir}

% Rencana jadwal pelaksanaan tugas akhir dapat dilihat pada gambar~\ref{jadwalTA1} dan gambar~\ref{jadwalTA2}.


% \begin{figure}[H]
%     \includestandalone[width=\textwidth]{figures/gantt-1}
%     \caption{
%         \label{jadwalTA1}
%         Jadwal pelaksanaan Tugas Akhir 1
%     }
% \end{figure}

% \begin{figure}[H]
%     \includestandalone[width=\textwidth]{figures/gantt-2}
%     \caption{
%         \label{jadwalTA2}
%         Jadwal pelaksanaan Tugas Akhir 1
%     }
% \end{figure}

\section{Sistematika Penulisan}
Laporan Tugas Akhir ini disusun dalam lima bab, yakni Pendahuluan, Studi Literatur, Rencana Penyelesaian Masalah, Implementasi dan Pengujian, serta Kesimpulan dan Saran.

Pendahuluan membahas gambaran latar belakang serta tujuan dari Tugas Akhir ini.

Dalam Studi Literatur akan dirangkum hasil studi literatur mengenai materi dan penelitian yang berkaitan dengan Tugas Akhir ini, yakni mengenai \emph{deep learning} dan pengoptimasi yang digunakan.

Dalam Rencana Penyelesaian Masalah akan dikaji masalah yang ada serta solusi yang akan dibuat dalam Tugas Akhir ini.

Dalam Implementasi dan Pengujian akan dibahas implementasi solusi serta hasil pengujian dan analisis hasil pengujian dari implementasi tersebut.

Dalam Kesimpulan dan Saran akan berisi kesimpulan yang didapatkan dalam pengerjaan Tugas Akhir serta saran yang dapat dilakukan untuk penelitian selanjutnya.
