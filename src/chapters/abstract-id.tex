\clearpage
\chapter*{ABSTRAK}
\addcontentsline{toc}{chapter}{ABSTRAK}

Teknik \emph{Deep Learning} saat ini banyak digunakan untuk penyelesaian masalah dalam berbagai bidang. Salah satu contoh yang populer pada saat laporan ini ditulis adalah ChatGPT, yang menggunakan model GPT milik OpenAI. Model \emph{deep learning} memiliki beberapa \emph{layer}, yang masing-masing terdiri atas beberapa \emph{parameter}. Pengembangan model \emph{deep learning} umumnya dilakukan dengan menambah jumlah parameter dalam model.

Jumlah \emph{parameter} yang semakin banyak akan memengaruhi jumlah komunikasi yang perlu dilakukan dalam \emph{deep learning} terdistribusi. Jumlah \emph{parameter} yang semakin banyak akan menyebabkan \emph{overhead} komunikasi meningkat. Oleh karena itu, tugas akhir ini akan membahas serta menggabungkan teknik-teknik yang ada untuk mengurangi jumlah komunikasi dan ukuran pembaruan parameter yang diperlukan untuk komunikasi dalam \emph{deep learning} terdistribusi. Teknik yang akan dikaji yakni teknik CADA dari \textcite{Chen2021CADA} serta Efficient-Adam dari \textcite{Chen2022Efficient}. Hasil penggabungan yang dilakukan memberikan pengurangan jumlah komunikasi hingga memerlukan hanya 0.97 kali ronde komunikasi yang diperlukan CADA, serta ukuran pembaruan parameter hanya 0.29 kali ukuran pembaruan parameter yang diperlukan CADA.

Kata Kunci: \emph{deep learning} terdistribusi, modifikasi pengoptimasi Adam, kompresi bobot, pengurangan komunikasi
\clearpage
