\clearpage
\chapter*{ABSTRAK}
\addcontentsline{toc}{chapter}{Abstrak}

%taruh abstrak bahasa indonesia di sini
Teknik \emph{Deep Learning} saat ini banyak digunakan untuk penyelesaian masalah dalam berbagai bidang. Salah satu contoh yang paling populer adalah ChatGPT, yang menggunakan model GPT milik OpenAI. Pembelajaran dalam \emph{deep learning} menggunakan proses stokastik atas \emph{dataset} yang ada untuk membuat aproksimasi fungsi tertentu. Model \emph{deep learning} memiliki beberapa \emph{layer}, yang masing-masing terdiri atas beberapa \emph{parameter}. Pengembangan model \emph{deep learning} umumnya dilakukan dengan menambah jumlah parameter dalam model.

Jumlah \emph{parameter} yang semakin banyak akan memengaruhi jumlah sumber daya yang diperlukan dalam pelatihan. Hal tersebut juga akan memengaruhi pembelajaran pada lingkungan terdistribusi. Pembelajaran dalam lingkungan terdistribusi memerlukan komunikasi, sehingga jumlah \emph{parameter} yang semakin banyak akan menyebabkan \emph{overhead} komunikasi meningkat.

Oleh karena itu, tugas akhir ini akan membahas serta menggabungkan teknik-teknik yang ada untuk mengurangi jumlah komunikasi dan ukuran data yang diperlukan untuk komunikasi dalam \emph{deep learning} terdistribusi. Teknik yang akan dikaji yakni teknik CADA dari \textcite{Chen2021CADA} serta Efficient-Adam dari \textcite{Chen2022Efficient}. Hasil penggabungan yang dilakuan memberikan pengurangan jumlah komunikasi yang lebih banyak dibandingkan CADA untuk model yang lebih besar, serta ukuran data yang cukup kecil dibandingkan teknik CADA.

Kata Kunci: \emph{deep learning} terdistribusi, modifikasi pengoptimasi Adam, kompresi bobot, pengurangan komunikasi
\clearpage
