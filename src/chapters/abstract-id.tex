\clearpage
\chapter*{ABSTRAK}
\addcontentsline{toc}{chapter}{ABSTRAK}

%taruh abstrak bahasa indonesia di sini
Teknik \emph{Deep Learning} saat ini banyak digunakan untuk penyelesaian masalah dalam berbagai bidang. Salah satu contoh yang paling populer adalah ChatGPT, yang menggunakan model GPT milik OpenAI. Pembelajaran dalam \emph{deep learning} menggunakan proses stokastik atas \emph{dataset} yang ada untuk membuat aproksimasi fungsi tertentu. Model \emph{deep learning} memiliki beberapa \emph{layer}, yang masing-masing terdiri atas beberapa \emph{parameter}. Pengembangan model \emph{deep learning} umumnya dilakukan dengan menambah jumlah parameter dalam model.

Jumlah \emph{parameter} yang semakin banyak akan memengaruhi jumlah sumber daya yang diperlukan dalam pelatihan. Hal tersebut juga akan memengaruhi pembelajaran pada lingkungan terdistribusi. Pembelajaran dalam lingkungan terdistribusi memerlukan komunikasi, sehingga jumlah \emph{parameter} yang semakin banyak akan menyebabkan \emph{overhead} komunikasi meningkat.

Oleh karena itu, tugas akhir ini akan membahas serta menggabungkan teknik-teknik yang ada untuk mengurangi jumlah komunikasi dan ukuran data yang diperlukan untuk komunikasi dalam \emph{deep learning} terdistribusi. Teknik yang akan dikaji yakni teknik CADA dari \textcite{Chen2021CADA} serta Efficient-Adam dari \textcite{Chen2022Efficient}. Hasil penggabungan yang dilakukan memberikan pengurangan jumlah komunikasi hingga memerlukan hanya 0.97 kali ronde komunikasi yang diperlukan CADA, serta ukuran data hanya 0.29 kali ukuran data yang diperlukan CADA.

Kata Kunci: \emph{deep learning} terdistribusi, modifikasi pengoptimasi Adam, kompresi bobot, pengurangan komunikasi
\clearpage
