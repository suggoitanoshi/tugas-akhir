\clearpage
\chapter*{ABSTRACT}
\addcontentsline{toc}{chapter}{ABSTRACT}
Deep learning is a technique used in many domains to solve various problems. One of the popular example at the time of writing is ChatGPT, which uses GPT model made by OpenAI. A deep learning model consists of several layers, each with its own parameters. More complex models are developed by increasing the number of learned parameters.

Increase of parameters in the model will cause communication overhead when learning in distributed architecture. Therefore, this final project will study about existing techniques to reduce communication overhead using reduction in communication rounds and compression. This final project will review CADA \parencite{Chen2021CADA} and Efficient-Adam \parencite{Chen2022Efficient}. Then, this final project will incorporate ideas from both techniques, resulting in a technique which could reduce communications to as much as 0.97 times the communication rounds of CADA and using only 0.29 times the communication size of CADA.

Index terms: distributed deep learning, communication modification of Adam optimizer, weight compression, communication reduction

\clearpage
