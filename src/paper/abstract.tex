\begin{abstract}
  Deep learning is a popular technique used to solve various problems in many domains. Deep learning models consists of learnable parameters. The development of more advanced deep learning models are headed towards increasing the number of parameters. Increase in the number of parameters will also increase communication overhead in distributed deep learning. Therefore, this paper will discuss about existing techniques to reduce communication overhead by reducing the communication rounds and compressing the communications. The reduction in communication uses the idea from CADA \cite{Chen2021CADA}, while the compression uses the idea from Efficient-Adam \cite{Chen2022Efficient}. The result are a technique which could reduce communication to as much as 0.97 times that of CADA, while reducing communication data size to 0.29 times the communication size of CADA.
\end{abstract}

\begin{IEEEkeywords}
  distributed deep learning, communication modification of Adam optimizer, weight compression, communication reduction
\end{IEEEkeywords}
